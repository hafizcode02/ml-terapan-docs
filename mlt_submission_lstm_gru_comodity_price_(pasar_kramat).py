# -*- coding: utf-8 -*-
"""MLT_Submission_LSTM_GRU_Comodity_Price_(Pasar_Kramat).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r1pwlB332rAsAeSAadHqxYw5pNzwZVXZ

## Pembuatan Model Deep Learning LSTM-GRU, Prediksi Harga Komoditas Pangan Pasar Kramat Kota Cirebon.

Authored by : Hafiz Caniago.

## Setup Variabel
"""

# Dapat Dirubah
## List Komoditas
comodity_list = ['bawang_merah', 'cabai_merah_besar', 'cabai_merah_keriting', 'cabai_rawit_hijau', 'cabai_rawit_merah'];
# Pilih Komoditas :
# 0 = bawang merah, 1 = cabai merah besar, 2 = cabai merah keriting,
# 3 = cabai rawit hijau, 4 = cabai rawit merah
comodity_selected = 0;

## Hyperparameter
set_max_epochs = 100; # 100, 150
set_neurons = 16; # 16, 32, 64
set_batch_size = 32; # 32

# Tidak Dirubah
set_test_size = 0.2; # Panjang Data Test
set_window_size = 30; # Panjang Data Pola Time Series

## Kode Tambahan
y_axis_title_comodity = ['Harga Bawang Merah', 'Harga Cabai Merah Besar',
                         'Harga Cabai Merah Keriting', 'Harga Cabai Rawit Hijau',
                         'Harga Cabai Rawit Merah']
chart_title = 'Historis Data ' + y_axis_title_comodity[comodity_selected]

"""## Import Library"""

# numpy & matplotlib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px

# Sklearn Lib
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, root_mean_squared_error

# Tensorflow
import tensorflow as tf
from keras.models import Model, load_model
from keras.layers import Input, Dense
from keras.layers import LSTM, GRU
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint

# JobLib
import joblib

# Export requirements.txt
!pip freeze > requirements.txt

"""## Data Preparation

"""

# Baca CSV File
df = pd.read_csv('/content/dataset.csv')

# Preview Data
df

# Dataset info
df.info()

df.describe()

"""> dari hasil deskripsi dari data, terdapat 5 kolom dataset komoditas pangan dengan masing masing data berjumlah 1097 baris.
- harga cabai merah besar
  - max : 110,000
  - min : 16,000
- harga cabai merah keriting
  - max : 105,000
  - min : 17,000
- harga cabai rawit hijau
  - max : 100,000
  - min : 17,000
- harga cabai rawit merah
  - max : 120,000
  - min : 16,000
- harga bawang merah
  - max : 70,000
  - min : 19,000

> Kode dibawah ini akan menstransformasikan kolom 'date' dari tipe data object ke tipe data date, serta melakukan konversi bentuk tanggal untuk 'date' jika tidak berformat 'Y-m-d'. serta melakukan konversi tipe data harga ke dalam bentuk float.
"""

# Mentransformasikan Data ke Format Datetime
df.date = pd.to_datetime(df.date, format = '%Y-%m-%d')
df.sort_values(by='date', ascending=True, inplace=True)
df.reset_index(drop=True, inplace=True)

# Mentransformasikan Data ke Format Integer (Harga Pangan tidak perlu koma)
columns_to_convert = df.columns[1:]  # Exclude the 'date' column
df[columns_to_convert] = df[columns_to_convert].astype('float64')

# Dataset info after transformation
df.info()

# Preview Data after transformation
df

# Panjang Data
print("Panjang Data : ", len(df))

# Check Duplicates data
df.duplicated(subset=['date'])

# Check Null Data
df.isnull().sum().sum()

"""### Preview Chart Comodity"""

# Ploting Chart Data Historis
fig = px.line(y=df[comodity_list[comodity_selected]], x=df.date)
fig.update_traces(line_color='black')
fig.update_layout(xaxis_title="Waktu Historis",
                  yaxis_title=y_axis_title_comodity[comodity_selected],
                  title={'text': chart_title, 'y':0.95, 'x':0.5, 'xanchor':'center', 'yanchor':'top'},
                  plot_bgcolor='rgba(255,223,0,0.8)')

"""### Normalisasi, Split Data Training & Testing, Pembentukan Pola Time Series

> Mempersiapkan instance scaler dan mengeluarkan file format scaler.pkl agar model bisa digunakan di luar google colab. (contoh : inference menggunakan model dan skalar yang sudah di export)
"""

# Data Scaling
scaler = MinMaxScaler()
scaler.fit(df[comodity_list[comodity_selected]].values.reshape(-1,1))

# Save the scaler
joblib.dump(scaler, 'scaler.pkl')

"""

> menyiapkan variable yang menampung jumlah atau ukuran data testing (20%)

"""

# Set Panjang Data Test
test_size = int(len(df) * set_test_size)
test_size

"""

> Melakukan visualisasi untuk menggambarkan seberapa banyak data training dan data testing

"""

# Ploting Data Training dan Test
plt.figure(figsize=(15, 6), dpi=150)
plt.rcParams['axes.facecolor'] = 'yellow'
plt.rc('axes',edgecolor='white')
plt.plot(df.date[:-test_size], df[comodity_list[comodity_selected]][:-test_size], color='black', lw=2)
plt.plot(df.date[-test_size:], df[comodity_list[comodity_selected]][-test_size:], color='blue', lw=2)
plt.title(y_axis_title_comodity[comodity_selected] + ' di Data Training dan Data Testing', fontsize=15)
plt.xlabel('Waktu', fontsize=12)
plt.ylabel('Harga', fontsize=12)
plt.legend(['Data Training', 'Data Testing'], loc='upper left', prop={'size': 15})
plt.grid(color='white')
plt.show()

"""- menyiapkan ukuran window_size
- menyiapkan data training (80%) data
- melakukan normalisasi pada data training dan membatasi angka koma menjadi 6 agar proses komputasi bisa lebih ringan.
- menyiapkan variable X_Train yang menyimpan data dalam bentuk window, dengan setiap window tersebut memiliki panjang 30 data (cek deklarasi di awal).
- menyiapkan variable y_train sebagai nilai aktual yang akan diprediksi menggunakan data pada setiap window.
"""

# Panjang data Loopback x hari
window_size = set_window_size

# Persiapkan data Training 80% dari data, dan normalisasikan
train_data = df[comodity_list[comodity_selected]][:-test_size]
train_data = scaler.transform(train_data.values.reshape(-1,1))

# Membulatkan hingga 6 angka di belakang koma
train_data = np.round(train_data, 6)

# Siapkan Variable untuk menampung data Train, disesuaikan dengan data loopback yaitu 30
X_train = []
y_train = []

for i in range(window_size, len(train_data)):
    X_train.append(train_data[i-30:i, 0])
    y_train.append(train_data[i, 0])

print("Panjang Data Train : ", len(train_data))
print("Panjang Data Train X - loopback : ", len(X_train))
print("Panjang Data Train y - loopback : ", len(y_train))

"""

> sama saja dengan proses pembentukan pada data training, pembagian data testing menggunakan 20% dari dataset

"""

# Persiapkan data test
test_data = df[comodity_list[comodity_selected]][-test_size-30:]
test_data = scaler.transform(test_data.values.reshape(-1,1))

# Membulatkan hingga 6 angka di belakang koma
test_data = np.round(test_data, 6)

# Siapkan Variable untuk menampung data Test, disesuaikan dengan data loopback yaitu 30
X_test = []
y_test = []

for i in range(window_size, len(test_data)):
    X_test.append(test_data[i-30:i, 0])
    y_test.append(test_data[i, 0])

print("Panjang Data Test : ", len(test_data))
print("Panjang Data Test X - loopback : ", len(X_test))
print("Panjang Data Test y - loopback : ", len(y_test))

"""

> setelahnya data tersebut akan dirubah menjadi bentuk array dan akan dilakukan reshape agar format datanya menjadi format tensor (format yang digunakan pada pelatihan deep learning)

"""

# Now X_train and X_test are nested lists (two-dimensional lists) and y_train is a one-dimensional list.
# We need to convert them to numpy arrays with a higher dimension,
# which is the data format accepted by TensorFlow when training the neural network:
X_train = np.array(X_train)
X_test  = np.array(X_test)
y_train = np.array(y_train)
y_test  = np.array(y_test)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test  = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
y_train = np.reshape(y_train, (-1,1))
y_test  = np.reshape(y_test, (-1,1))

print('X_train Shape: ', X_train.shape)
print('y_train Shape: ', y_train.shape)
print('X_test Shape:  ', X_test.shape)
print('y_test Shape:  ', y_test.shape)

"""## Training Model

> Training model dilakukan dengan arsitektur sebagai berikut :
"""

# Create LSTM + GRU Model
def define_model():
    input1 = Input(shape=(window_size,1))
    x = LSTM(units = set_neurons, return_sequences=True)(input1)
    x = GRU(units = set_neurons)(x)  # Setting return_sequences=False to get 2D output
    dnn_output = Dense(1)(x)

    model = Model(inputs=input1, outputs=[dnn_output])
    model.compile(loss='mean_squared_error', optimizer=Adam())
    model.summary()

    return model

"""

> Proses training model dilakukan juga dengan memanfaatkan model checkpoint agar model terbaik dapat diperoleh dari pelatihan dan disimpan dalam bentuk h5 file.

"""

# Training Model
model = define_model()
# Define the ModelCheckpoint callback
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)
# Fit the model with the checkpoint callback
history = model.fit(X_train, y_train, epochs=set_max_epochs, batch_size=set_batch_size, validation_split=0.1, verbose=1, callbacks=[checkpoint])
# Load the best model saved during training
model = load_model('best_model.h5')

"""## Evaluasi Model

> Evaluasi model dilakukan pada data yang masih dalam bentuk normalisasi dan menggunakan metriks MSE, RMSE, MAPE, dan Accuracy
"""

# Model Evaluation (Data Normalisasi)
result = model.evaluate(X_test, y_test)
y_pred = model.predict(X_test)
# Membulatkan angka koma normalisasi maksimal 6
y_pred = np.round(y_pred, 6)

# Calculate Evaluation Value of Model
MAPE = mean_absolute_percentage_error(y_test, y_pred)
RMSE = root_mean_squared_error(y_test, y_pred)
Accuracy = 1 - MAPE

print("Test Loss (MSE):", result) # MSE
print("Test RMSE:", RMSE)
print("Test MAPE:", MAPE)
print("Test Accuracy:", Accuracy)

"""

> Evaluasi dilakukan lagi, namun kali ini saya ingin mencoba melihat evaluasinya dalam bentuk data yang sudah di denormalisasi, hal ini saya lakukan untuk mengetahui hasil evaluasi jika pada data ril

"""

# Kalkulasi Evaluasi Model (Data Denormalisasi / Asli)
y_test_true_dn = np.round(scaler.inverse_transform(y_test)) # Dibulatkan
y_test_pred_dn = np.round(scaler.inverse_transform(y_pred)) # Dibulatkan

mse = mean_squared_error(y_test_true_dn, y_test_pred_dn)
rmse = root_mean_squared_error(y_test_true_dn, y_test_pred_dn)
mape = mean_absolute_percentage_error(y_test_true_dn, y_test_pred_dn) * 100

print(f"Test MSE on denormalized data: {mse}")
print(f"Test RMSE on denormalized data: {rmse}")
print(f"Test MAPE on denormalized data: {mape}%")

"""

> melakukan visualiasi untuk melihat bagaimana performa model untuk memprediksi

"""

# Visualizing Results

# Mengembalikan nilai yang dinormalisasi ke data yang asli
y_test_true = np.round(scaler.inverse_transform(y_test))
y_test_pred = np.round(scaler.inverse_transform(y_pred))
real_train_data = scaler.inverse_transform(train_data)

# Menggambar Plot
plt.figure(figsize=(15, 6), dpi=150)
plt.rcParams['axes.facecolor'] = 'yellow'
plt.rc('axes',edgecolor='white')
plt.plot(df.date.iloc[:-test_size], real_train_data, color='black', lw=2)
plt.plot(df.date.iloc[-test_size:], y_test_true, color='blue', lw=2)
plt.plot(df.date.iloc[-test_size:], y_test_pred, color='red', lw=2)
plt.title('Performa Model untuk Prediksi ' + y_axis_title_comodity[comodity_selected], fontsize=15)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Price', fontsize=12)
plt.legend(['Data Training', 'Data Test Asli', 'Data Test Prediksi'], loc='upper left', prop={'size': 15})
plt.grid(color='white')
plt.show()

"""

> melakukan visualiasi untuk melihat bagaimana performa model untuk memprediksi (tampilkan data test only)

"""

# Menggambar Plot Data Test Only
plt.figure(figsize=(15, 6), dpi=150)
plt.rcParams['axes.facecolor'] = 'yellow'
plt.rc('axes',edgecolor='white')
plt.plot(df.date.iloc[-test_size:], y_test_true, color='blue', lw=2)
plt.plot(df.date.iloc[-test_size:], y_test_pred, color='red', lw=2)
plt.title('Performa Model untuk Prediksi (Data Test) ' + y_axis_title_comodity[comodity_selected], fontsize=15)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Price', fontsize=12)
plt.legend(['Data Test Asli', 'Data Test Prediksi'], loc='upper left', prop={'size': 15})
plt.grid(color='white')
plt.show()

"""

> pada proses ini saya gunakan untuk melihat bagaimana persentase kesalahan hasil prediksi dengan nilai aktual

"""

# Data Perbandingan dalam bentuk tabel
flatent_test_true = [item[0] for item in y_test_true]
flatent_test_pred = [item[0] for item in y_test_pred]

table_data = pd.DataFrame({
    'Date': df['date'].iloc[-test_size:],
    'Harga Asli': flatent_test_true,
    'Harga Prediksi': flatent_test_pred,
})

# Calculate the Percentage Error (often used in Mean Absolute Percentage Error or MAPE)
table_data['Persentase Error'] = (table_data['Harga Prediksi']) / (table_data['Harga Asli']  * 100)

table_data

"""## Test Prediksi (Model Inference)"""

# Memprediksi X Hari Kedepan
target = df[comodity_list[comodity_selected]].tail(180).values.reshape(-1, 1)
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_target = scaler.fit_transform(target)

# Membulatkan hingga 4 angka di belakang koma
scaled_target = np.round(scaled_target, 6)

# Predict future values
future_steps = 3  # Set the number of future steps you want to predict
future_data = scaled_target[-window_size:, :]

print(future_data.reshape((1, window_size, 1)))

future_predictions = []
for _ in range(future_steps):
    # Reshape the data to match the input shape of the model
    future_input = future_data.reshape((1, window_size, 1))

    # Make the prediction
    future_pred = model.predict(future_input)

    # Append the prediction to the results
    future_predictions.append(future_pred[0, 0])

    # Update the input data for the next prediction
    future_data = np.vstack((future_data[1:], future_pred))

# Inverse transform the predictions to the original scale
future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))

print(future_predictions)

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(target, label='True Values')
plt.plot(np.arange(len(target) , len(target) + future_steps), future_predictions, label='Future Predictions')
plt.legend()
plt.show()